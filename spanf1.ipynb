{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import utils.NERutils as nu\n",
    "\n",
    "def toSpans(tags):\n",
    "    spans = set()\n",
    "    for beg in range(len(tags)):\n",
    "        if tags[beg][0] == 'B':\n",
    "            end = beg\n",
    "            for end in range(beg+1, len(tags)):\n",
    "                if tags[beg][0] != 'I':\n",
    "                    break\n",
    "            spans.add(str(beg) + '-' + str(end) + ':' + tags[beg][2:])\n",
    "    return spans\n",
    "\n",
    "\n",
    "def getF1ScoreFromPath(predPath: str, goldPath: str):\n",
    "    gold = nu.readDataset(goldPath)\n",
    "    pred =  nu.readDataset(predPath)\n",
    "    goldEnts = nu.getEntsForPredictions(gold)\n",
    "    predEnts =  nu.getEntsForPredictions(pred)\n",
    "    entScores = []\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for goldEnt, predEnt in zip(goldEnts, predEnts):\n",
    "        goldSpans = toSpans(goldEnt)\n",
    "        predSpans = toSpans(predEnt)\n",
    "        overlap = len(goldSpans.intersection(predSpans))\n",
    "        tp += overlap\n",
    "        fp += len(predSpans) - overlap\n",
    "        fn += len(goldSpans) - overlap\n",
    "        \n",
    "    prec = 0.0 if tp+fp == 0 else tp/(tp+fp)\n",
    "    rec = 0.0 if tp+fn == 0 else tp/(tp+fn)\n",
    "    f1 = 0.0 if prec+rec == 0.0 else 2 * (prec * rec) / (prec + rec)\n",
    "    return f1\n",
    "\n",
    "def getF1ScoreFromLists(golds:list, preds: list):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for goldEnt, predEnt in zip(golds, preds):\n",
    "        goldSpans = toSpans(goldEnt)\n",
    "        predSpans = toSpans(predEnt)\n",
    "        overlap = len(goldSpans.intersection(predSpans))\n",
    "        tp += overlap\n",
    "        fp += len(predSpans) - overlap\n",
    "        fn += len(goldSpans) - overlap\n",
    "        \n",
    "    prec = 0.0 if tp+fp == 0 else tp/(tp+fp)\n",
    "    rec = 0.0 if tp+fn == 0 else tp/(tp+fn)\n",
    "    f1 = 0.0 if prec+rec == 0.0 else 2 * (prec * rec) / (prec + rec)\n",
    "    return f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getF1ScoreFromPath(\"data/BIOdev.parquet\", \"data/BIOdev.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.bert.modeling_bert import BertModel\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel\n",
    "\n",
    "# Timetracker\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=3, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "class BertForTokenClassification(BertPreTrainedModel):\n",
    "    config_class = BertConfig\n",
    "\n",
    "    def __init__(self, config, tags, patience=3, delta=0, verbose=False):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = len(tags)\n",
    "        \n",
    "        # Load model body\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        \n",
    "        # Set up token classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "        \n",
    "        # Load and initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "        # Define patience and delta for early stopping\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Save accuracy and loss\n",
    "        self.training_acc = []\n",
    "        self.training_loss = []\n",
    "\n",
    "        self.validation_acc = []\n",
    "        self.validation_loss = []\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        labels=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # Use model body to get encoder representations\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            **kwargs\n",
    "        )\n",
    "        # Apply classifier to encoder representation\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "        # Calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        # Return model output object\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "    \n",
    "    def train_loop(self, data_loader, device, optimizer):\n",
    "        self.train()\n",
    "\n",
    "        # Initialize parameters for calculating training loss and accuracy\n",
    "        num_batches = len(data_loader)\n",
    "        size = len(data_loader.dataset)\n",
    "        epoch_loss, correct = 0, 0\n",
    "\n",
    "        for idx, batch in enumerate(tqdm(data_loader)):\n",
    "            ids = batch[\"input_ids\"].to(device, dtype=torch.long)\n",
    "            mask = batch[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "            targets = batch[\"labels\"].to(device, dtype=torch.long)\n",
    "            \n",
    "            outputs = self.forward(input_ids = ids,\n",
    "                            attention_mask = mask,\n",
    "                            labels = targets)\n",
    "            \n",
    "            loss, tr_logits = outputs.loss, outputs.logits\n",
    "\n",
    "            # Flatten targets and predictions\n",
    "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = tr_logits.view(-1, self.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            \n",
    "            # Mask predictions and targets (includes [CLS] and [SEP] token predictions)\n",
    "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate train loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            correct += (targets == predictions).type(torch.float).sum().item()\n",
    "        \n",
    "        # Caluclate training loss and accuracy for the current epoch\n",
    "        train_loss = epoch_loss/num_batches\n",
    "        train_acc = correct/size\n",
    "        \n",
    "        # Save loss and accuracy to history\n",
    "        self.training_loss.append(train_loss)\n",
    "        self.training_acc.append(train_acc)\n",
    "\n",
    "    def val_loop(self, data_loader, device):\n",
    "        self.eval()\n",
    "\n",
    "        # Initialize parameters for calculating training loss and accuracy\n",
    "        num_batches = len(data_loader)\n",
    "        size = len(data_loader.dataset)\n",
    "        epoch_loss, correct = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(data_loader):\n",
    "                \n",
    "                ids = batch[\"input_ids\"].to(device, dtype=torch.long)\n",
    "                mask = batch[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "                targets = batch[\"labels\"].to(device, dtype=torch.long)\n",
    "                \n",
    "                outputs = self.forward(input_ids = ids,\n",
    "                                attention_mask = mask,\n",
    "                                labels = targets)\n",
    "                \n",
    "                # Save validation loss\n",
    "                loss, tr_logits = outputs.loss, outputs.logits\n",
    "\n",
    "                # Flatten targets and predictions\n",
    "                flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "                active_logits = tr_logits.view(-1, self.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "                flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "                \n",
    "                # Mask predictions and targets (includes [CLS] and [SEP] token predictions)\n",
    "                active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "                targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "                predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "                # Calculate train loss and accuracy\n",
    "                epoch_loss += loss.item()\n",
    "                correct += (targets == predictions).type(torch.float).sum().item()\n",
    "        \n",
    "        # Caluclate training loss and accuracy for the current epoch\n",
    "        val_loss = epoch_loss/num_batches\n",
    "        val_acc = correct/size\n",
    "        \n",
    "        # Save loss and accuracy to history\n",
    "        self.validation_loss.append(val_loss)\n",
    "        self.validation_acc.append(val_acc)\n",
    "\n",
    "    def fit(self, num_epochs, data_loader, device, optimizer):\n",
    "        \n",
    "        early_stopping = EarlyStopping(patience=self.patience, verbose=self.verbose, delta=self.delta)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch+1} of {num_epochs} epochs\")\n",
    "           \n",
    "            self.train_loop(data_loader, device, optimizer)\n",
    "            self.val_loop(data_loader, device)\n",
    "            \n",
    "            # Early stopping\n",
    "            early_stopping(self.validation_loss[-1], self)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        print(\"Done!\")\n",
    "\n",
    "    def test(self, data_loader, device):\n",
    "\n",
    "        self.val_loop(data_loader, device)\n",
    "\n",
    "\n",
    "    def predict(self, data_loader, device):\n",
    "        self.eval()\n",
    "\n",
    "        # Initialize parameters for calculating training loss and accuracy\n",
    "        logits = []\n",
    "        masks = []\n",
    "        indices = []\n",
    "        size = len(data_loader.dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in tqdm(enumerate(data_loader), total=size):\n",
    "                \n",
    "                ids = batch[\"input_ids\"].to(device, dtype=torch.long)\n",
    "                mask = batch[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "                # targets = batch[\"labels\"].to(device, dtype=torch.long)\n",
    "                \n",
    "                outputs = self(ids, mask)\n",
    "                \n",
    "                # Save validation loss\n",
    "                # print(type(outputs[0]))\n",
    "                # outputs = [o[torch.nonzero(m)] for o,m in zip(outputs[0], mask)]\n",
    "                # print(outputs[0].shape, mask.shape)\n",
    "                # print(torch.nonzero(mask))\n",
    "                logits.extend(outputs[0])\n",
    "                masks.extend(mask)\n",
    "                indices.extend(batch[\"index\"])\n",
    "\n",
    "        return (logits, masks, indices)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from model_new import BertForTokenClassification\n",
    "import utils.NERutils as nu\n",
    "import utils.query_funcs as q\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "# Define tokenizer\n",
    "bert_model_name = \"bert-base-multilingual-cased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "train_path = \"data/BIOtrain.parquet\"\n",
    "dev_path = \"data/BIOdev.parquet\"\n",
    "test_path = \"data/BIOtest.parquet\"\n",
    "\n",
    "filter = 'Legal'\n",
    "\n",
    "train_dataset = nu.NERdataset(dataset_path=train_path, tokenizer=bert_tokenizer, filter=filter)\n",
    "dev_dataset = nu.NERdataset(dataset_path=dev_path, tokenizer=bert_tokenizer)\n",
    "test_dataset = nu.NERdataset(dataset_path=test_path, tokenizer=bert_tokenizer, filter=filter)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"cpu\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "# Config\n",
    "bert_model_name = \"bert-base-multilingual-cased\"\n",
    "bert_config = AutoConfig.from_pretrained(\n",
    "    bert_model_name, \n",
    "    num_labels=len(train_dataset.tags), \n",
    "    id2label=train_dataset.index2tag, \n",
    "    label2id=train_dataset.tag2index\n",
    ")\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(bert_model_name, config=bert_config, tags=train_dataset.tags, verbose=True).to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model.load_state_dict(torch.load(\"Trained_models/checkpoint_trained_model.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(dev_loader):\n",
    "        ids = batch[\"input_ids\"].to(device, dtype=torch.long)\n",
    "        mask = batch[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "        targets = batch[\"labels\"].to(device, dtype=torch.long)\n",
    "        \n",
    "        outputs = model.forward(input_ids = ids,\n",
    "                        attention_mask = mask,\n",
    "                        labels = targets)\n",
    "        \n",
    "        # Save validation loss\n",
    "        loss, tr_logits = outputs.loss, outputs.logits\n",
    "\n",
    "        # Flatten targets and predictions\n",
    "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        \n",
    "        # Mask predictions and targets (includes [CLS] and [SEP] token predictions)\n",
    "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "        targets_2 = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "        # Calculate train loss and accuracy\n",
    "        break\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [*map(train_dataset.index2tag.get, list(predictions.cpu().numpy()))]\n",
    "golds = [*map(train_dataset.index2tag.get, list(targets_2.cpu().numpy()))]\n",
    "\n",
    "getF1ScoreFromLists(golds=golds, preds=preds)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-WORK OF ART', 'B-WORK OF ART', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERCENT', 'O', 'O', 'O', 'B-ORGANIZATION', 'B-ORGANIZATION', 'B-ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LANGUAGE', 'I-LANGUAGE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORDINAL', 'I-ORDINAL', 'I-PERCENT', 'I-PERCENT', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'B-ORGANIZATION', 'B-ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MONEY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MONEY', 'I-MONEY', 'I-MONEY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MONEY', 'I-MONEY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NORP', 'B-NORP', 'B-NORP', 'B-NORP', 'O', 'O', 'O', 'O', 'I-MONEY', 'I-MONEY', 'I-MONEY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(golds)\n",
    "print(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
