{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.NERutils as nu\n",
    "\n",
    "def toSpans(tags):\n",
    "    spans = set()\n",
    "    for beg in range(len(tags)):\n",
    "        if tags[beg][0] == 'B':\n",
    "            end = beg\n",
    "            for end in range(beg+1, len(tags)):\n",
    "                if tags[beg][0] != 'I':\n",
    "                    break\n",
    "            spans.add(str(beg) + '-' + str(end) + ':' + tags[beg][2:])\n",
    "    return spans\n",
    "\n",
    "\n",
    "def getInstanceScores(predPath, goldPath):\n",
    "    gold = nu.readDataset(goldPath)\n",
    "    pred =  nu.readDataset(predPath)\n",
    "    goldEnts = nu.getEntsForPredictions(gold)\n",
    "    predEnts =  nu.getEntsForPredictions(pred)\n",
    "    entScores = []\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for goldEnt, predEnt in zip(goldEnts, predEnts):\n",
    "        goldSpans = toSpans(goldEnt)\n",
    "        predSpans = toSpans(predEnt)\n",
    "        overlap = len(goldSpans.intersection(predSpans))\n",
    "        tp += overlap\n",
    "        fp += len(predSpans) - overlap\n",
    "        fn += len(goldSpans) - overlap\n",
    "        \n",
    "    prec = 0.0 if tp+fp == 0 else tp/(tp+fp)\n",
    "    rec = 0.0 if tp+fn == 0 else tp/(tp+fn)\n",
    "    f1 = 0.0 if prec+rec == 0.0 else 2 * (prec * rec) / (prec + rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getInstanceScores(\"data/BIOdev.parquet\", \"data/BIOdev.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.11.6)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.bert.modeling_bert import BertModel\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel\n",
    "\n",
    "# Timetracker\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BertForTokenClassification(BertPreTrainedModel):\n",
    "    config_class = BertConfig\n",
    "\n",
    "    def __init__(self, config, tags, patience=3, delta=0, verbose=False):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = len(tags)\n",
    "        \n",
    "        # Load model body\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        \n",
    "        # Set up token classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "        \n",
    "        # Load and initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "        # Define patience and delta for early stopping\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Save accuracy and loss\n",
    "        self.training_acc = []\n",
    "        self.training_loss = []\n",
    "\n",
    "        self.validation_acc = []\n",
    "        self.validation_loss = []\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        labels=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # Use model body to get encoder representations\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            **kwargs\n",
    "        )\n",
    "        # Apply classifier to encoder representation\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "        # Calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        # Return model output object\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "    \n",
    "    def train_loop(self, data_loader, device, optimizer):\n",
    "        self.train()\n",
    "\n",
    "        # Initialize parameters for calculating training loss and accuracy\n",
    "        num_batches = len(data_loader)\n",
    "        size = len(data_loader.dataset)\n",
    "        epoch_loss, correct = 0, 0\n",
    "\n",
    "        for idx, batch in enumerate(tqdm(data_loader)):\n",
    "            ids = batch[\"input_ids\"].to(device, dtype=torch.long)\n",
    "            mask = batch[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "            targets = batch[\"labels\"].to(device, dtype=torch.long)\n",
    "            \n",
    "            outputs = self.forward(input_ids = ids,\n",
    "                            attention_mask = mask,\n",
    "                            labels = targets)\n",
    "            \n",
    "            loss, tr_logits = outputs.loss, outputs.logits\n",
    "\n",
    "            # Flatten targets and predictions\n",
    "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = tr_logits.view(-1, self.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            \n",
    "            # Mask predictions and targets (includes [CLS] and [SEP] token predictions)\n",
    "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate train loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            correct += (targets == predictions).type(torch.float).sum().item()\n",
    "        \n",
    "        # Caluclate training loss and accuracy for the current epoch\n",
    "        train_loss = epoch_loss/num_batches\n",
    "        train_acc = correct/size\n",
    "        \n",
    "        # Save loss and accuracy to history\n",
    "        self.training_loss.append(train_loss)\n",
    "        self.training_acc.append(train_acc)\n",
    "\n",
    "    def val_loop(self, data_loader, device):\n",
    "        self.eval()\n",
    "\n",
    "        # Initialize parameters for calculating training loss and accuracy\n",
    "        num_batches = len(data_loader)\n",
    "        size = len(data_loader.dataset)\n",
    "        epoch_loss, correct = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(data_loader):\n",
    "                \n",
    "                ids = batch[\"input_ids\"].to(device, dtype=torch.long)\n",
    "                mask = batch[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "                targets = batch[\"labels\"].to(device, dtype=torch.long)\n",
    "                \n",
    "                outputs = self.forward(input_ids = ids,\n",
    "                                attention_mask = mask,\n",
    "                                labels = targets)\n",
    "                \n",
    "                # Save validation loss\n",
    "                loss, tr_logits = outputs.loss, outputs.logits\n",
    "\n",
    "                # Flatten targets and predictions\n",
    "                flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "                active_logits = tr_logits.view(-1, self.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "                flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "                \n",
    "                # Mask predictions and targets (includes [CLS] and [SEP] token predictions)\n",
    "                active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "                targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "                predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "                # Calculate train loss and accuracy\n",
    "                epoch_loss += loss.item()\n",
    "                correct += (targets == predictions).type(torch.float).sum().item()\n",
    "        \n",
    "        # Caluclate training loss and accuracy for the current epoch\n",
    "        val_loss = epoch_loss/num_batches\n",
    "        val_acc = correct/size\n",
    "        \n",
    "        # Save loss and accuracy to history\n",
    "        self.validation_loss.append(val_loss)\n",
    "        self.validation_acc.append(val_acc)\n",
    "\n",
    "    def fit(self, num_epochs, data_loader, device, optimizer):\n",
    "        \n",
    "        early_stopping = EarlyStopping(patience=self.patience, verbose=self.verbose, delta=self.delta)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {epoch+1} of {num_epochs} epochs\")\n",
    "           \n",
    "            self.train_loop(data_loader, device, optimizer)\n",
    "            self.val_loop(data_loader, device)\n",
    "            \n",
    "            # Early stopping\n",
    "            early_stopping(self.validation_loss[-1], self)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        print(\"Done!\")\n",
    "\n",
    "    def test(self, data_loader, device):\n",
    "\n",
    "        self.val_loop(data_loader, device)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
