{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=False\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joke/anaconda3/envs/NLP2024/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers import DistilBertModel\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel\n",
    "\n",
    "import NERutils as NU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bert_model_name = \"distilbert-base-multilingual-cased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "trainNER = NU.NERdataset(\"data/train.parquet\", bert_tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pretrained Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"transformers_version\": \"4.39.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['bert.embeddings.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.word_embeddings.weight', 'bert.transformer.layer.0.attention.k_lin.bias', 'bert.transformer.layer.0.attention.k_lin.weight', 'bert.transformer.layer.0.attention.out_lin.bias', 'bert.transformer.layer.0.attention.out_lin.weight', 'bert.transformer.layer.0.attention.q_lin.bias', 'bert.transformer.layer.0.attention.q_lin.weight', 'bert.transformer.layer.0.attention.v_lin.bias', 'bert.transformer.layer.0.attention.v_lin.weight', 'bert.transformer.layer.0.ffn.lin1.bias', 'bert.transformer.layer.0.ffn.lin1.weight', 'bert.transformer.layer.0.ffn.lin2.bias', 'bert.transformer.layer.0.ffn.lin2.weight', 'bert.transformer.layer.0.output_layer_norm.bias', 'bert.transformer.layer.0.output_layer_norm.weight', 'bert.transformer.layer.0.sa_layer_norm.bias', 'bert.transformer.layer.0.sa_layer_norm.weight', 'bert.transformer.layer.1.attention.k_lin.bias', 'bert.transformer.layer.1.attention.k_lin.weight', 'bert.transformer.layer.1.attention.out_lin.bias', 'bert.transformer.layer.1.attention.out_lin.weight', 'bert.transformer.layer.1.attention.q_lin.bias', 'bert.transformer.layer.1.attention.q_lin.weight', 'bert.transformer.layer.1.attention.v_lin.bias', 'bert.transformer.layer.1.attention.v_lin.weight', 'bert.transformer.layer.1.ffn.lin1.bias', 'bert.transformer.layer.1.ffn.lin1.weight', 'bert.transformer.layer.1.ffn.lin2.bias', 'bert.transformer.layer.1.ffn.lin2.weight', 'bert.transformer.layer.1.output_layer_norm.bias', 'bert.transformer.layer.1.output_layer_norm.weight', 'bert.transformer.layer.1.sa_layer_norm.bias', 'bert.transformer.layer.1.sa_layer_norm.weight', 'bert.transformer.layer.2.attention.k_lin.bias', 'bert.transformer.layer.2.attention.k_lin.weight', 'bert.transformer.layer.2.attention.out_lin.bias', 'bert.transformer.layer.2.attention.out_lin.weight', 'bert.transformer.layer.2.attention.q_lin.bias', 'bert.transformer.layer.2.attention.q_lin.weight', 'bert.transformer.layer.2.attention.v_lin.bias', 'bert.transformer.layer.2.attention.v_lin.weight', 'bert.transformer.layer.2.ffn.lin1.bias', 'bert.transformer.layer.2.ffn.lin1.weight', 'bert.transformer.layer.2.ffn.lin2.bias', 'bert.transformer.layer.2.ffn.lin2.weight', 'bert.transformer.layer.2.output_layer_norm.bias', 'bert.transformer.layer.2.output_layer_norm.weight', 'bert.transformer.layer.2.sa_layer_norm.bias', 'bert.transformer.layer.2.sa_layer_norm.weight', 'bert.transformer.layer.3.attention.k_lin.bias', 'bert.transformer.layer.3.attention.k_lin.weight', 'bert.transformer.layer.3.attention.out_lin.bias', 'bert.transformer.layer.3.attention.out_lin.weight', 'bert.transformer.layer.3.attention.q_lin.bias', 'bert.transformer.layer.3.attention.q_lin.weight', 'bert.transformer.layer.3.attention.v_lin.bias', 'bert.transformer.layer.3.attention.v_lin.weight', 'bert.transformer.layer.3.ffn.lin1.bias', 'bert.transformer.layer.3.ffn.lin1.weight', 'bert.transformer.layer.3.ffn.lin2.bias', 'bert.transformer.layer.3.ffn.lin2.weight', 'bert.transformer.layer.3.output_layer_norm.bias', 'bert.transformer.layer.3.output_layer_norm.weight', 'bert.transformer.layer.3.sa_layer_norm.bias', 'bert.transformer.layer.3.sa_layer_norm.weight', 'bert.transformer.layer.4.attention.k_lin.bias', 'bert.transformer.layer.4.attention.k_lin.weight', 'bert.transformer.layer.4.attention.out_lin.bias', 'bert.transformer.layer.4.attention.out_lin.weight', 'bert.transformer.layer.4.attention.q_lin.bias', 'bert.transformer.layer.4.attention.q_lin.weight', 'bert.transformer.layer.4.attention.v_lin.bias', 'bert.transformer.layer.4.attention.v_lin.weight', 'bert.transformer.layer.4.ffn.lin1.bias', 'bert.transformer.layer.4.ffn.lin1.weight', 'bert.transformer.layer.4.ffn.lin2.bias', 'bert.transformer.layer.4.ffn.lin2.weight', 'bert.transformer.layer.4.output_layer_norm.bias', 'bert.transformer.layer.4.output_layer_norm.weight', 'bert.transformer.layer.4.sa_layer_norm.bias', 'bert.transformer.layer.4.sa_layer_norm.weight', 'bert.transformer.layer.5.attention.k_lin.bias', 'bert.transformer.layer.5.attention.k_lin.weight', 'bert.transformer.layer.5.attention.out_lin.bias', 'bert.transformer.layer.5.attention.out_lin.weight', 'bert.transformer.layer.5.attention.q_lin.bias', 'bert.transformer.layer.5.attention.q_lin.weight', 'bert.transformer.layer.5.attention.v_lin.bias', 'bert.transformer.layer.5.attention.v_lin.weight', 'bert.transformer.layer.5.ffn.lin1.bias', 'bert.transformer.layer.5.ffn.lin1.weight', 'bert.transformer.layer.5.ffn.lin2.bias', 'bert.transformer.layer.5.ffn.lin2.weight', 'bert.transformer.layer.5.output_layer_norm.bias', 'bert.transformer.layer.5.output_layer_norm.weight', 'bert.transformer.layer.5.sa_layer_norm.bias', 'bert.transformer.layer.5.sa_layer_norm.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized because the shapes did not match:\n",
      "- distilbert.embeddings.word_embeddings.weight: found shape torch.Size([119547, 768]) in the checkpoint and torch.Size([30522, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=19, bias=True)\n",
       "  (bert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import model\n",
    "from transformers import DistilBertConfig\n",
    "# bert_config = AutoConfig.from_pretrained(\n",
    "#     bert_model_name, num_labels=len(trainNER.tags), id2label=trainNER.index2tag, label2id=trainNER.tag2index\n",
    "# )\n",
    "\n",
    "bert_config = DistilBertConfig()\n",
    "print(bert_config)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "model.BertForTokenClassification.from_pretrained(\n",
    "    bert_model_name, config=bert_config, tags=trainNER.tags, ignore_mismatched_sizes=True\n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    # Get tokens with special characters\n",
    "    tokens = tokenizer(text).tokens()\n",
    "    # Encode the sequence into IDs\n",
    "    input_ids = bert_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # Get predictions as distribution over 7 possible classes\n",
    "    outputs = model(input_ids)[0]\n",
    "    # Take argmax to get most likely class per token\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    # Convert to DataFrame\n",
    "    preds = [tags[p] for p in predictions[0].cpu().numpy()]\n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Texts for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = bert_tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Dataset as tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    labels_list, preds_list = [], []\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            # Ignore label IDs = -100\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                example_labels.append(trainNER.index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(trainNER.index2tag[preds[batch_idx][seq_idx]])\n",
    "\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "\n",
    "    return preds_list, labels_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': 8,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 6,\n",
    "                'pin_memory': True\n",
    "                }\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(trainNER, **train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions, eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return model.BertForTokenClassification.from_pretrained(\n",
    "        bert_model_name, config=bert_config, tags=trainNER.tags, ignore_mismatched_sizes=True\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['bert.embeddings.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.word_embeddings.weight', 'bert.transformer.layer.0.attention.k_lin.bias', 'bert.transformer.layer.0.attention.k_lin.weight', 'bert.transformer.layer.0.attention.out_lin.bias', 'bert.transformer.layer.0.attention.out_lin.weight', 'bert.transformer.layer.0.attention.q_lin.bias', 'bert.transformer.layer.0.attention.q_lin.weight', 'bert.transformer.layer.0.attention.v_lin.bias', 'bert.transformer.layer.0.attention.v_lin.weight', 'bert.transformer.layer.0.ffn.lin1.bias', 'bert.transformer.layer.0.ffn.lin1.weight', 'bert.transformer.layer.0.ffn.lin2.bias', 'bert.transformer.layer.0.ffn.lin2.weight', 'bert.transformer.layer.0.output_layer_norm.bias', 'bert.transformer.layer.0.output_layer_norm.weight', 'bert.transformer.layer.0.sa_layer_norm.bias', 'bert.transformer.layer.0.sa_layer_norm.weight', 'bert.transformer.layer.1.attention.k_lin.bias', 'bert.transformer.layer.1.attention.k_lin.weight', 'bert.transformer.layer.1.attention.out_lin.bias', 'bert.transformer.layer.1.attention.out_lin.weight', 'bert.transformer.layer.1.attention.q_lin.bias', 'bert.transformer.layer.1.attention.q_lin.weight', 'bert.transformer.layer.1.attention.v_lin.bias', 'bert.transformer.layer.1.attention.v_lin.weight', 'bert.transformer.layer.1.ffn.lin1.bias', 'bert.transformer.layer.1.ffn.lin1.weight', 'bert.transformer.layer.1.ffn.lin2.bias', 'bert.transformer.layer.1.ffn.lin2.weight', 'bert.transformer.layer.1.output_layer_norm.bias', 'bert.transformer.layer.1.output_layer_norm.weight', 'bert.transformer.layer.1.sa_layer_norm.bias', 'bert.transformer.layer.1.sa_layer_norm.weight', 'bert.transformer.layer.2.attention.k_lin.bias', 'bert.transformer.layer.2.attention.k_lin.weight', 'bert.transformer.layer.2.attention.out_lin.bias', 'bert.transformer.layer.2.attention.out_lin.weight', 'bert.transformer.layer.2.attention.q_lin.bias', 'bert.transformer.layer.2.attention.q_lin.weight', 'bert.transformer.layer.2.attention.v_lin.bias', 'bert.transformer.layer.2.attention.v_lin.weight', 'bert.transformer.layer.2.ffn.lin1.bias', 'bert.transformer.layer.2.ffn.lin1.weight', 'bert.transformer.layer.2.ffn.lin2.bias', 'bert.transformer.layer.2.ffn.lin2.weight', 'bert.transformer.layer.2.output_layer_norm.bias', 'bert.transformer.layer.2.output_layer_norm.weight', 'bert.transformer.layer.2.sa_layer_norm.bias', 'bert.transformer.layer.2.sa_layer_norm.weight', 'bert.transformer.layer.3.attention.k_lin.bias', 'bert.transformer.layer.3.attention.k_lin.weight', 'bert.transformer.layer.3.attention.out_lin.bias', 'bert.transformer.layer.3.attention.out_lin.weight', 'bert.transformer.layer.3.attention.q_lin.bias', 'bert.transformer.layer.3.attention.q_lin.weight', 'bert.transformer.layer.3.attention.v_lin.bias', 'bert.transformer.layer.3.attention.v_lin.weight', 'bert.transformer.layer.3.ffn.lin1.bias', 'bert.transformer.layer.3.ffn.lin1.weight', 'bert.transformer.layer.3.ffn.lin2.bias', 'bert.transformer.layer.3.ffn.lin2.weight', 'bert.transformer.layer.3.output_layer_norm.bias', 'bert.transformer.layer.3.output_layer_norm.weight', 'bert.transformer.layer.3.sa_layer_norm.bias', 'bert.transformer.layer.3.sa_layer_norm.weight', 'bert.transformer.layer.4.attention.k_lin.bias', 'bert.transformer.layer.4.attention.k_lin.weight', 'bert.transformer.layer.4.attention.out_lin.bias', 'bert.transformer.layer.4.attention.out_lin.weight', 'bert.transformer.layer.4.attention.q_lin.bias', 'bert.transformer.layer.4.attention.q_lin.weight', 'bert.transformer.layer.4.attention.v_lin.bias', 'bert.transformer.layer.4.attention.v_lin.weight', 'bert.transformer.layer.4.ffn.lin1.bias', 'bert.transformer.layer.4.ffn.lin1.weight', 'bert.transformer.layer.4.ffn.lin2.bias', 'bert.transformer.layer.4.ffn.lin2.weight', 'bert.transformer.layer.4.output_layer_norm.bias', 'bert.transformer.layer.4.output_layer_norm.weight', 'bert.transformer.layer.4.sa_layer_norm.bias', 'bert.transformer.layer.4.sa_layer_norm.weight', 'bert.transformer.layer.5.attention.k_lin.bias', 'bert.transformer.layer.5.attention.k_lin.weight', 'bert.transformer.layer.5.attention.out_lin.bias', 'bert.transformer.layer.5.attention.out_lin.weight', 'bert.transformer.layer.5.attention.q_lin.bias', 'bert.transformer.layer.5.attention.q_lin.weight', 'bert.transformer.layer.5.attention.v_lin.bias', 'bert.transformer.layer.5.attention.v_lin.weight', 'bert.transformer.layer.5.ffn.lin1.bias', 'bert.transformer.layer.5.ffn.lin1.weight', 'bert.transformer.layer.5.ffn.lin2.bias', 'bert.transformer.layer.5.ffn.lin2.weight', 'bert.transformer.layer.5.output_layer_norm.bias', 'bert.transformer.layer.5.output_layer_norm.weight', 'bert.transformer.layer.5.sa_layer_norm.bias', 'bert.transformer.layer.5.sa_layer_norm.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized because the shapes did not match:\n",
      "- distilbert.embeddings.word_embeddings.weight: found shape torch.Size([119547, 768]) in the checkpoint and torch.Size([30522, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# trainer.is_model_parallel\n",
    "model = model_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 66/1471 [00:50<14:24,  1.63it/s] "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "N_EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "def run_epoch(epoch):\n",
    "    model.train()\n",
    "\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "\n",
    "    for idx, batch in enumerate(tqdm(training_loader)):\n",
    "        ids = batch[\"input_ids\"].to(device, dtype=torch.long)\n",
    "        mask = batch[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "        targets = batch[\"labels\"].to(device, dtype=torch.long)\n",
    "        \n",
    "        outputs = model(input_ids = ids,\n",
    "                        attention_mask = mask,\n",
    "                        labels = targets)\n",
    "        \n",
    "        loss, tr_logits = outputs.loss, outputs.logits\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    run_epoch(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>Jeff</td>\n",
       "      <td>Dean</td>\n",
       "      <td>ist</td>\n",
       "      <td>ein</td>\n",
       "      <td>Info</td>\n",
       "      <td>##rmat</td>\n",
       "      <td>##iker</td>\n",
       "      <td>bei</td>\n",
       "      <td>Google</td>\n",
       "      <td>in</td>\n",
       "      <td>Kalifornien</td>\n",
       "      <td>,</td>\n",
       "      <td>syn</td>\n",
       "      <td>##tes</td>\n",
       "      <td>jeg</td>\n",
       "      <td>n</td>\n",
       "      <td>##æste</td>\n",
       "      <td>år</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>PERCENT</td>\n",
       "      <td>QUANTITY</td>\n",
       "      <td>O</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>O</td>\n",
       "      <td>WORK OF ART</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>ORDINAL</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1     2    3    4        5         6       7         8   \\\n",
       "Tokens  [CLS]  Jeff  Dean  ist  ein     Info    ##rmat  ##iker       bei   \n",
       "Tags        O     O     O    O    O  PERCENT  QUANTITY       O  LOCATION   \n",
       "\n",
       "            9            10           11 12   13       14   15 16      17  18  \\\n",
       "Tokens  Google           in  Kalifornien  ,  syn    ##tes  jeg  n  ##æste  år   \n",
       "Tags         O  WORK OF ART            O  O    O  ORDINAL    O  O       O   O   \n",
       "\n",
       "           19  \n",
       "Tokens  [SEP]  \n",
       "Tags        O  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide_output\n",
    "text_de = (\n",
    "    \"Jeff Dean ist ein Informatiker bei Google in Kalifornien, syntes jeg næste år\"\n",
    ")\n",
    "tag_text(text_de, trainNER.tags, model, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
