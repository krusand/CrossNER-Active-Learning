{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read ewt Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "bert_model_name = \"bert-base-multilingual-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_iob2_with_metadata(file_path):\n",
    "    documents = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        document = []\n",
    "        metadata = {}\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('#'):\n",
    "                if line.startswith('# text'):\n",
    "                    metadata['text'] = line.split('=')[1].strip()  \n",
    "                continue\n",
    "            if not line:  \n",
    "                if document: \n",
    "                    documents.append((metadata, document))\n",
    "                    document = []  \n",
    "                    metadata = {}  \n",
    "            else:\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) >= 4:\n",
    "                    token = parts[1]\n",
    "                    ner_tag = parts[2]\n",
    "                    document.append((token, ner_tag))\n",
    "        if document:  \n",
    "            documents.append((metadata, document))\n",
    "    return documents\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "training_data = read_iob2_with_metadata('data/ewt_data/en_ewt-ud-train.iob2')\n",
    "dev_data = read_iob2_with_metadata('data/ewt_data/en_ewt-ud-dev.iob2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_df(data):\n",
    "    sentences = []\n",
    "    tags = []\n",
    "\n",
    "    for line in data:\n",
    "        sentences.append(line[0][\"text\"])\n",
    "        prev_tag = ''\n",
    "        for i,word_tag in enumerate(line[1]):\n",
    "            _, tag = word_tag\n",
    "            if i == 0:\n",
    "                prev_tag += tag\n",
    "            else:\n",
    "                prev_tag += \",\" + tag\n",
    "\n",
    "        tags.append(prev_tag)\n",
    "\n",
    "\n",
    "    return pd.DataFrame({\"sentence\": sentences, \"tags\": tags})\n",
    "\n",
    "training_df = convert_to_df(training_data)\n",
    "dev_df = convert_to_df(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label2id = {k: v for v, k in enumerate(pd.unique(','.join(training_df[\"tags\"].values).split(',')))}\n",
    "id2label = {v: k for v, k in enumerate(pd.unique(','.join(training_df[\"tags\"].values).split(',')))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = [\"[CLS]\"]\n",
    "    labels = [\"O\"]\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    tokenized_sentence.append(\"[SEP]\")\n",
    "    labels.append(\"O\")\n",
    "\n",
    "\n",
    "    return tokenized_sentence, labels\n",
    "\n",
    "def get_attn_mask(tokenized_sentence):\n",
    "    return [1 if tok != \"[PAD]\" else 0 for tok in tokenized_sentence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proper_data_structure(df):\n",
    "    df[\"tokenized_sentence\"] = df.apply(lambda x: tokenize_and_preserve_labels(x[\"sentence\"],x[\"tags\"], tokenizer)[0],axis=1)\n",
    "    df[\"tokenized_sentence_tags\"] = df.apply(lambda x: tokenize_and_preserve_labels(x[\"sentence\"],x[\"tags\"], tokenizer)[1],axis=1)\n",
    "\n",
    "    df[\"ids\"] = df.apply(lambda x: tokenizer.convert_tokens_to_ids(x[\"tokenized_sentence\"]), axis=1)\n",
    "    df[\"attn_mask\"] = df.apply(lambda x: get_attn_mask(x[\"tokenized_sentence\"]), axis=1)\n",
    "    df[\"targets\"] = df.apply(lambda x: [label2id[label] for label in x[\"tokenized_sentence_tags\"]],axis=1)\n",
    "    return df\n",
    "\n",
    "training_df = get_proper_data_structure(training_df)\n",
    "dev_df = get_proper_data_structure(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>attn_mask</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 23525, 10106, 10105, 11356, 10124, 146, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 146, 20337, 13078, 23118, 102]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[0, 1, 1, 1, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 21660, 10454, 14289, 10114, 10347, 10464...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[101, 10117, 12672, 10108, 10105, 35017, 10124...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 12613, 10105, 42230, 57667, 37158, 10376...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12538</th>\n",
       "      <td>[101, 28140, 169, 16745, 10635, 18571, 31391, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12539</th>\n",
       "      <td>[101, 156, 119, 10111, 146, 10529, 10151, 2474...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12540</th>\n",
       "      <td>[101, 10117, 10399, 10525, 10114, 32949, 10111...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12541</th>\n",
       "      <td>[101, 21200, 11131, 117, 15127, 20104, 117, 22...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12542</th>\n",
       "      <td>[101, 146, 112, 181, 56011, 10908, 10114, 2376...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12543 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     ids   \n",
       "0      [101, 23525, 10106, 10105, 11356, 10124, 146, ...  \\\n",
       "1                   [101, 146, 20337, 13078, 23118, 102]   \n",
       "2      [101, 21660, 10454, 14289, 10114, 10347, 10464...   \n",
       "3      [101, 10117, 12672, 10108, 10105, 35017, 10124...   \n",
       "4      [101, 12613, 10105, 42230, 57667, 37158, 10376...   \n",
       "...                                                  ...   \n",
       "12538  [101, 28140, 169, 16745, 10635, 18571, 31391, ...   \n",
       "12539  [101, 156, 119, 10111, 146, 10529, 10151, 2474...   \n",
       "12540  [101, 10117, 10399, 10525, 10114, 32949, 10111...   \n",
       "12541  [101, 21200, 11131, 117, 15127, 20104, 117, 22...   \n",
       "12542  [101, 146, 112, 181, 56011, 10908, 10114, 2376...   \n",
       "\n",
       "                                               attn_mask   \n",
       "0                      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \\\n",
       "1                                     [1, 1, 1, 1, 1, 1]   \n",
       "2      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                  ...   \n",
       "12538  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "12539  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "12540  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "12541                        [1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "12542  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                 targets  \n",
       "0                      [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0]  \n",
       "1                                     [0, 1, 1, 1, 2, 0]  \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                  ...  \n",
       "12538  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "12539  [0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "12540  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "12541                        [0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "12542  [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[12543 rows x 3 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df[[\"ids\", \"attn_mask\", \"targets\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_torch_dataset(df):\n",
    "    data_set = []\n",
    "\n",
    "    for ids, attn, targets in df[[\"ids\",\"attn_mask\", \"targets\"]].iloc:\n",
    "        data_set.append({\"input_ids\":ids,\n",
    "                            \"attention_mask\":attn,\n",
    "                            \"labels\":targets})\n",
    "    return data_set\n",
    "\n",
    "training_set = get_torch_dataset(training_df)\n",
    "dev_set = get_torch_dataset(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': 8,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 1\n",
    "                }\n",
    "\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_set, **train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', \n",
    "                                                   num_labels=len(id2label),\n",
    "                                                   id2label=id2label,\n",
    "                                                   label2id=label2id)\n",
    "\n",
    "from transformers import AutoConfig\n",
    "\n",
    "bert_config = AutoConfig.from_pretrained(bert_model_name, \n",
    "                                         num_labels=len(id2label),\n",
    "                                         id2label=id2label, label2id=label2id)\n",
    "def model_init():\n",
    "    return (BertForTokenClassification\n",
    "            .from_pretrained(bert_model_name, config=bert_config)\n",
    "            .to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "num_epochs = 0.1\n",
    "batch_size = 8\n",
    "logging_steps = len(training_set) // batch_size\n",
    "model_name = f\"{bert_model_name}-finetuned_ewt\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name, log_level=\"error\", num_train_epochs=num_epochs, \n",
    "    per_device_train_batch_size=batch_size, \n",
    "    per_device_eval_batch_size=batch_size, evaluation_strategy=\"epoch\", \n",
    "    save_steps=1e6, weight_decay=0.01, disable_tqdm=False, \n",
    "    logging_steps=logging_steps, push_to_hub=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pyarrow as pa\n",
    "\n",
    "training_set = pd.DataFrame(training_set)\n",
    "dev_set = pd.DataFrame(dev_set)\n",
    "\n",
    "training_set_pa = Dataset(pa.Table.from_pandas(training_set))\n",
    "dev_set_pa = Dataset(pa.Table.from_pandas(dev_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model_init=model_init, args=training_args, \n",
    "                  data_collator=data_collator,\n",
    "                  train_dataset=training_set_pa,\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joke/.pyenv/versions/3.10.5/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab99332eb6347588eb73b5f905e4592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Trainer: evaluation requires an eval_dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/transformers/trainer.py:1455\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1455\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   1458\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   1459\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/transformers/trainer.py:1565\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1563\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 1565\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, epoch, metrics)\n\u001b[1;32m   1568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/transformers/trainer.py:2204\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2201\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m-> 2204\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_eval_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2205\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2207\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.5/lib/python3.10/site-packages/transformers/trainer.py:717\u001b[0m, in \u001b[0;36mTrainer.get_eval_dataloader\u001b[0;34m(self, eval_dataset)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;124;03mReturns the evaluation [`~torch.utils.data.DataLoader`].\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;124;03m        the `model.forward()` method are automatically removed. It must implement `__len__`.\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer: evaluation requires an eval_dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    718\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m eval_dataset \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, datasets\u001b[38;5;241m.\u001b[39mDataset):\n",
      "\u001b[0;31mValueError\u001b[0m: Trainer: evaluation requires an eval_dataset."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-ORG',\n",
       " 'B-ORG',\n",
       " 'B-ORG',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'B-ORG',\n",
       " 'B-ORG',\n",
       " 'B-LOC',\n",
       " 'I-ORG',\n",
       " 'I-LOC',\n",
       " 'B-LOC',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'B-PER',\n",
       " 'B-PER',\n",
       " 'B-ORG',\n",
       " 'I-PER',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'I-PER',\n",
       " 'B-ORG',\n",
       " 'I-LOC',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'B-PER',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'I-LOC']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = training_set[2][\"ids\"].unsqueeze(0)\n",
    "mask = training_set[2][\"mask\"].unsqueeze(0)\n",
    "targets = training_set[2][\"targets\"].unsqueeze(0)\n",
    "\n",
    "output = model(input_ids=ids, attention_mask = mask, labels= targets)\n",
    "\n",
    "\n",
    "[id2label[pred] for pred in torch.argmax(output[1],dim=2).cpu().numpy()[0]]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
