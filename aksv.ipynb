{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read ewt Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import pyarrow as pa\n",
    "\n",
    "bert_model_name = \"bert-base-multilingual-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_iob2_with_metadata(file_path):\n",
    "    documents = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        document = []\n",
    "        metadata = {}\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('#'):\n",
    "                if line.startswith('# text'):\n",
    "                    metadata['text'] = line.split('=')[1].strip()  \n",
    "                continue\n",
    "            if not line:  \n",
    "                if document: \n",
    "                    documents.append((metadata, document))\n",
    "                    document = []  \n",
    "                    metadata = {}  \n",
    "            else:\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) >= 4:\n",
    "                    token = parts[1]\n",
    "                    ner_tag = parts[2]\n",
    "                    document.append((token, ner_tag))\n",
    "        if document:  \n",
    "            documents.append((metadata, document))\n",
    "    return documents\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "training_data = read_iob2_with_metadata('data/ewt_data/en_ewt-ud-train.iob2')\n",
    "dev_data = read_iob2_with_metadata('data/ewt_data/en_ewt-ud-dev.iob2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_df(data):\n",
    "    sentences = []\n",
    "    tags = []\n",
    "\n",
    "    for line in data:\n",
    "        sentences.append(line[0][\"text\"])\n",
    "        prev_tag = ''\n",
    "        for i,word_tag in enumerate(line[1]):\n",
    "            _, tag = word_tag\n",
    "            if i == 0:\n",
    "                prev_tag += tag\n",
    "            else:\n",
    "                prev_tag += \",\" + tag\n",
    "\n",
    "        tags.append(prev_tag)\n",
    "\n",
    "\n",
    "    return pd.DataFrame({\"sentence\": sentences, \"tags\": tags})\n",
    "\n",
    "training_df = convert_to_df(training_data)\n",
    "dev_df = convert_to_df(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label2id = {k: v for v, k in enumerate(pd.unique(','.join(training_df[\"tags\"].values).split(',')))}\n",
    "id2label = {v: k for v, k in enumerate(pd.unique(','.join(training_df[\"tags\"].values).split(',')))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = [\"[CLS]\"]\n",
    "    labels = [\"O\"]\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    tokenized_sentence.append(\"[SEP]\")\n",
    "    labels.append(\"O\")\n",
    "\n",
    "\n",
    "    return tokenized_sentence, labels\n",
    "\n",
    "def get_attn_mask(tokenized_sentence):\n",
    "    return [1 if tok != \"[PAD]\" else 0 for tok in tokenized_sentence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proper_data_structure(df):\n",
    "    df[\"tokenized_sentence\"] = df.apply(lambda x: tokenize_and_preserve_labels(x[\"sentence\"],x[\"tags\"], tokenizer)[0],axis=1)\n",
    "    df[\"tokenized_sentence_tags\"] = df.apply(lambda x: tokenize_and_preserve_labels(x[\"sentence\"],x[\"tags\"], tokenizer)[1],axis=1)\n",
    "\n",
    "    df[\"input_ids\"] = df.apply(lambda x: tokenizer.convert_tokens_to_ids(x[\"tokenized_sentence\"]), axis=1)\n",
    "    df[\"attention_mask\"] = df.apply(lambda x: get_attn_mask(x[\"tokenized_sentence\"]), axis=1)\n",
    "    df[\"labels\"] = df.apply(lambda x: [label2id[label] for label in x[\"tokenized_sentence_tags\"]],axis=1)\n",
    "    return df\n",
    "\n",
    "training_df = get_proper_data_structure(training_df)\n",
    "dev_df = get_proper_data_structure(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 23525, 10106, 10105, 11356, 10124, 146, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 146, 20337, 13078, 23118, 102]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[0, 1, 1, 1, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 21660, 10454, 14289, 10114, 10347, 10464...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[101, 10117, 12672, 10108, 10105, 35017, 10124...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 12613, 10105, 42230, 57667, 37158, 10376...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12538</th>\n",
       "      <td>[101, 28140, 169, 16745, 10635, 18571, 31391, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12539</th>\n",
       "      <td>[101, 156, 119, 10111, 146, 10529, 10151, 2474...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12540</th>\n",
       "      <td>[101, 10117, 10399, 10525, 10114, 32949, 10111...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12541</th>\n",
       "      <td>[101, 21200, 11131, 117, 15127, 20104, 117, 22...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12542</th>\n",
       "      <td>[101, 146, 112, 181, 56011, 10908, 10114, 2376...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12543 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input_ids   \n",
       "0      [101, 23525, 10106, 10105, 11356, 10124, 146, ...  \\\n",
       "1                   [101, 146, 20337, 13078, 23118, 102]   \n",
       "2      [101, 21660, 10454, 14289, 10114, 10347, 10464...   \n",
       "3      [101, 10117, 12672, 10108, 10105, 35017, 10124...   \n",
       "4      [101, 12613, 10105, 42230, 57667, 37158, 10376...   \n",
       "...                                                  ...   \n",
       "12538  [101, 28140, 169, 16745, 10635, 18571, 31391, ...   \n",
       "12539  [101, 156, 119, 10111, 146, 10529, 10151, 2474...   \n",
       "12540  [101, 10117, 10399, 10525, 10114, 32949, 10111...   \n",
       "12541  [101, 21200, 11131, 117, 15127, 20104, 117, 22...   \n",
       "12542  [101, 146, 112, 181, 56011, 10908, 10114, 2376...   \n",
       "\n",
       "                                          attention_mask   \n",
       "0                      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \\\n",
       "1                                     [1, 1, 1, 1, 1, 1]   \n",
       "2      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                  ...   \n",
       "12538  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "12539  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "12540  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "12541                        [1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "12542  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                  labels  \n",
       "0                      [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0]  \n",
       "1                                     [0, 1, 1, 1, 2, 0]  \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                  ...  \n",
       "12538  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "12539  [0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "12540  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "12541                        [0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "12542  [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[12543 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df[[\"input_ids\", \"attention_mask\", \"labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.DataFrame(training_df[[\"input_ids\", \"attention_mask\", \"labels\"]])\n",
    "dev_set = pd.DataFrame(dev_df[[\"input_ids\", \"attention_mask\", \"labels\"]])\n",
    "\n",
    "training_set_pa = Dataset(pa.Table.from_pandas(training_set))\n",
    "dev_set_pa = Dataset(pa.Table.from_pandas(dev_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from transformers import BertForTokenClassification\n",
    "\n",
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "bert_config = AutoConfig.from_pretrained(bert_model_name, \n",
    "                                         num_labels=len(id2label),\n",
    "                                         id2label=id2label, label2id=label2id)\n",
    "def model_init():\n",
    "    return (BertForTokenClassification\n",
    "            .from_pretrained(bert_model_name, config=bert_config)\n",
    "            .to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 8\n",
    "logging_steps = len(training_set) // batch_size\n",
    "model_name = f\"{bert_model_name}-finetuned_ewt\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name, log_level=\"error\", num_train_epochs=num_epochs, \n",
    "    per_device_train_batch_size=batch_size, \n",
    "    per_device_eval_batch_size=batch_size, evaluation_strategy=\"epoch\", \n",
    "    save_steps=1e6, weight_decay=0.01, disable_tqdm=False, \n",
    "    logging_steps=logging_steps, push_to_hub=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator_new(data):\n",
    "    data = data_collator(data)\n",
    "    for key in data:\n",
    "        data[key] = data[key].to(device)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return (BertForTokenClassification\n",
    "            .from_pretrained(\"model_weights\", config=bert_config)\n",
    "            .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model_init=model_init, args=training_args, \n",
    "                  data_collator = data_collator_new,\n",
    "                  train_dataset = training_set_pa,\n",
    "                  eval_dataset = dev_set_pa,\n",
    "                  tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>Jeff</td>\n",
       "      <td>Dean</td>\n",
       "      <td>ist</td>\n",
       "      <td>ein</td>\n",
       "      <td>Info</td>\n",
       "      <td>##rmat</td>\n",
       "      <td>##iker</td>\n",
       "      <td>bei</td>\n",
       "      <td>Google</td>\n",
       "      <td>in</td>\n",
       "      <td>Kalifornien</td>\n",
       "      <td>,</td>\n",
       "      <td>syn</td>\n",
       "      <td>##tes</td>\n",
       "      <td>jeg</td>\n",
       "      <td>n</td>\n",
       "      <td>##Ã¦ste</td>\n",
       "      <td>Ã¥r</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0      1      2    3    4     5       6       7    8       9   10   \n",
       "Tokens  [CLS]   Jeff   Dean  ist  ein  Info  ##rmat  ##iker  bei  Google  in  \\\n",
       "Tags        O  B-PER  I-PER    O    O     O       O       O    O       O   O   \n",
       "\n",
       "                 11     12   13     14   15 16      17  18     19  \n",
       "Tokens  Kalifornien      ,  syn  ##tes  jeg  n  ##Ã¦ste  Ã¥r  [SEP]  \n",
       "Tags          B-LOC  B-LOC    O      O    O  O       O   O      O  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    # Get tokens with special characters\n",
    "    tokens = tokenizer(text).tokens()\n",
    "    # Encode the sequence into IDs\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids #.to(device)\n",
    "    # Get predictions as distribution over 7 possible classes\n",
    "    outputs = model(input_ids)[0]\n",
    "    # Take argmax to get most likely class per token\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    # Convert to DataFrame\n",
    "    preds = [tags[p] for p in predictions[0].cpu().numpy()]\n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])\n",
    "\n",
    "# hide_output\n",
    "text_de = (\n",
    "    \"Jeff Dean ist ein Informatiker bei Google in Kalifornien, syntes jeg nÃ¦ste Ã¥r\"\n",
    ")\n",
    "\n",
    "tags = [\"o\"] * 10\n",
    "tag_text(text_de, list(label2id.keys()), trainer.model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>I</td>\n",
       "      <td>##gua</td>\n",
       "      <td>##zu</td>\n",
       "      <td>Falls</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2      3      4      5\n",
       "Tokens  [CLS]      I  ##gua   ##zu  Falls  [SEP]\n",
       "Tags        O  B-LOC  B-LOC  B-LOC  I-LOC      O"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_text(training_df.iloc[1]['sentence'], list(label2id.keys()), trainer.model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-LOC', 'B-LOC', 'B-LOC', 'I-LOC', 'O']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.iloc[1]['tokenized_sentence_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"model_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
