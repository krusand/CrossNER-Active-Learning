{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "train = pd.read_parquet(\"data/train-00000-of-00001.parquet\")\n",
    "dev = pd.read_parquet(\"data/dev-00000-of-00001.parquet\")\n",
    "test = pd.read_parquet(\"data/test-00000-of-00001.parquet\")\n",
    "\n",
    "\n",
    "### convert to Huggingface dataset\n",
    "train_dataset = Dataset(pa.Table.from_pandas(train))\n",
    "dev_dataset = Dataset(pa.Table.from_pandas(dev))\n",
    "test_dataset = Dataset(pa.Table.from_pandas(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'EVENT',\n",
       " 'TIME',\n",
       " 'CARDINAL',\n",
       " 'QUANTITY',\n",
       " 'GPE',\n",
       " 'NORP',\n",
       " 'DATE',\n",
       " 'PRODUCT',\n",
       " 'FACILITY',\n",
       " 'LOCATION',\n",
       " 'MONEY',\n",
       " 'LANGUAGE',\n",
       " 'PERSON',\n",
       " 'WORK OF ART',\n",
       " 'PERCENT',\n",
       " 'LAW',\n",
       " 'ORDINAL',\n",
       " 'ORGANIZATION']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = [\"O\"] + list(set([x[0][\"label\"] for x in train_dataset[\"ents\"] if x]))\n",
    "index2tag = {idx: tag for idx, tag in enumerate(tags)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_model_name = \"bert-base-multilingual-cased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.bert.modeling_bert import BertModel\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel\n",
    "\n",
    "\n",
    "class BertForTokenClassification(BertPreTrainedModel):\n",
    "    config_class = BertConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = len(tags)\n",
    "        # Load model body\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        # Set up token classification head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "        # Load and initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, \n",
    "                labels=None, **kwargs):\n",
    "        # Use model body to get encoder representations\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids, **kwargs)\n",
    "        # Apply classifier to encoder representation\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        logits = self.classifier(sequence_output)\n",
    "        # Calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        # Return model output object\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, \n",
    "                                     hidden_states=outputs.hidden_states, \n",
    "                                     attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pretrained Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "from transformers import AutoConfig\n",
    "\n",
    "bert_config = AutoConfig.from_pretrained(bert_model_name, \n",
    "                                         num_labels=len(tags),\n",
    "                                         id2label=index2tag, label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"mps\")\n",
    "\n",
    "bert_model = (BertForTokenClassification\n",
    "              .from_pretrained(bert_model_name, config=bert_config)\n",
    "              .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    # Get tokens with special characters\n",
    "    tokens = tokenizer(text).tokens()\n",
    "    # Encode the sequence into IDs\n",
    "    input_ids = bert_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    # Get predictions as distribution over 7 possible classes\n",
    "    outputs = model(input_ids)[0]\n",
    "    # Take argmax to get most likely class per token\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    # Convert to DataFrame\n",
    "    preds = [tags[p] for p in predictions[0].cpu().numpy()]\n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Texts for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = bert_tokenizer(examples[\"tokens\"], truncation=True, \n",
    "                                      is_split_into_words=True)\n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Dataset as tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "def encode_line(line):\n",
    "    ents = line[\"ents\"]\n",
    "    tokenized = bert_tokenizer(line[\"text\"])\n",
    "    labels = [\"O\"]\n",
    "\n",
    "    word_start = 0\n",
    "    for word in bert_tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"]):\n",
    "        if word in (\"[CLS]\", \"[SEP]\"):\n",
    "            continue\n",
    "        if word.startswith(\"##\"):\n",
    "            word = word[2:]\n",
    "        \n",
    "        word_start += line[\"text\"][word_start:].find(word)\n",
    "\n",
    "        if ents:\n",
    "            if word_start >= ents[0][\"start\"] and word_start <= ents[0][\"end\"]:\n",
    "                labels.append(ents[0][\"label\"])\n",
    "            else:\n",
    "                labels.append(\"O\")\n",
    "            if ents[0][\"end\"] <= word_start + len(word):\n",
    "                ents = ents[1:]\n",
    "        else:\n",
    "            labels.append(\"O\")\n",
    "\n",
    "        # print(line[\"text\"][word_start: word_start+len(word)])\n",
    "        word_start += len(word)\n",
    "\n",
    "    labels = [tag2index[x] for x in labels + [\"O\"]]\n",
    "    tokenized[\"labels\"] = labels\n",
    "    # print(pd.DataFrame([i for i in zip(bert_tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"]),labels)]))\n",
    "    return tokenized\n",
    "\n",
    "#  encode_line(train_dataset[2])\n",
    "\n",
    "train_dataset = pd.DataFrame([encode_line(l) for l in train_dataset])\n",
    "dev_dataset = pd.DataFrame([encode_line(l) for l in dev_dataset])\n",
    "test_dataset = pd.DataFrame([encode_line(l) for l in test_dataset])\n",
    "                      \n",
    "train_dataset = Dataset(pa.Table.from_pandas(train_dataset))\n",
    "dev_dataset = Dataset(pa.Table.from_pandas(dev_dataset))\n",
    "test_dataset = Dataset(pa.Table.from_pandas(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    labels_list, preds_list = [], []\n",
    "\n",
    "    for batch_idx in range(batch_size):\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len):\n",
    "            # Ignore label IDs = -100\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "\n",
    "        labels_list.append(example_labels)\n",
    "        preds_list.append(example_preds)\n",
    "\n",
    "    return preds_list, labels_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "num_epochs = 0.1\n",
    "batch_size = 30\n",
    "logging_steps = len(train_dataset) // batch_size\n",
    "model_name = f\"{bert_model_name}-finetuned-panx-de\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name, log_level=\"error\", num_train_epochs=num_epochs, \n",
    "    per_device_train_batch_size=batch_size, \n",
    "    per_device_eval_batch_size=batch_size, evaluation_strategy=\"epoch\", \n",
    "    save_steps=1e6, weight_decay=0.01, disable_tqdm=False, \n",
    "    logging_steps=logging_steps, push_to_hub=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions, \n",
    "                                       eval_pred.label_ids)\n",
    "    return {\"f1\": f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return (BertForTokenClassification\n",
    "            .from_pretrained(bert_model_name, config=bert_config)\n",
    "            .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model_init=model_init, args=training_args, \n",
    "                  data_collator=data_collator, compute_metrics=compute_metrics,\n",
    "                  train_dataset=train_dataset,\n",
    "                  eval_dataset=dev_dataset, \n",
    "                  tokenizer=bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joke/.pyenv/versions/3.10.5/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25167b23a6349459d8eaab2030fbe51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149f382468934b3f960ceb8e05eabd75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.45122504234313965, 'eval_f1': 0.16574585635359115, 'eval_runtime': 24.5047, 'eval_samples_per_second': 59.621, 'eval_steps_per_second': 2.0, 'epoch': 0.1}\n",
      "{'train_runtime': 90.2683, 'train_samples_per_second': 13.03, 'train_steps_per_second': 0.443, 'train_loss': 0.7313706398010253, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joke/.pyenv/versions/3.10.5/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PERSON seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/joke/.pyenv/versions/3.10.5/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NORP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/joke/.pyenv/versions/3.10.5/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ORGANIZATION seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/joke/.pyenv/versions/3.10.5/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CARDINAL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/joke/.pyenv/versions/3.10.5/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: GPE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/joke/.pyenv/versions/3.10.5/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DATE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/joke/.pyenv/versions/3.10.5/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LOCATION seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/joke/.pyenv/versions/3.10.5/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: FACILITY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/joke/.pyenv/versions/3.10.5/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: WORK OF ART seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/joke/.pyenv/versions/3.10.5/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: TIME seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/joke/.pyenv/versions/3.10.5/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRODUCT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/joke/.pyenv/versions/3.10.5/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: MONEY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/joke/.pyenv/versions/3.10.5/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PERCENT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/joke/.pyenv/versions/3.10.5/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ORDINAL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/joke/.pyenv/versions/3.10.5/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: QUANTITY seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/joke/.pyenv/versions/3.10.5/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LANGUAGE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/joke/.pyenv/versions/3.10.5/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LAW seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/joke/.pyenv/versions/3.10.5/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: EVENT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40, training_loss=0.7313706398010253, metrics={'train_runtime': 90.2683, 'train_samples_per_second': 13.03, 'train_steps_per_second': 0.443, 'train_loss': 0.7313706398010253, 'epoch': 0.1})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'eval_loss': 0.45122504234313965,\n",
       "  'eval_f1': 0.16574585635359115,\n",
       "  'eval_runtime': 24.5047,\n",
       "  'eval_samples_per_second': 59.621,\n",
       "  'eval_steps_per_second': 2.0,\n",
       "  'epoch': 0.1,\n",
       "  'step': 40},\n",
       " {'train_runtime': 90.2683,\n",
       "  'train_samples_per_second': 13.03,\n",
       "  'train_steps_per_second': 0.443,\n",
       "  'total_flos': 40103987437620.0,\n",
       "  'train_loss': 0.7313706398010253,\n",
       "  'epoch': 0.1,\n",
       "  'step': 40}]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.state.log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>Jeff</td>\n",
       "      <td>Dean</td>\n",
       "      <td>ist</td>\n",
       "      <td>ein</td>\n",
       "      <td>Info</td>\n",
       "      <td>##rmat</td>\n",
       "      <td>##iker</td>\n",
       "      <td>bei</td>\n",
       "      <td>Google</td>\n",
       "      <td>in</td>\n",
       "      <td>Kalifornien</td>\n",
       "      <td>,</td>\n",
       "      <td>syn</td>\n",
       "      <td>##tes</td>\n",
       "      <td>jeg</td>\n",
       "      <td>n</td>\n",
       "      <td>##æste</td>\n",
       "      <td>år</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <td>O</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0       1       2    3    4     5       6       7    8    \n",
       "Tokens  [CLS]    Jeff    Dean  ist  ein  Info  ##rmat  ##iker  bei  \\\n",
       "Tags        O  PERSON  PERSON    O    O     O       O       O    O   \n",
       "\n",
       "                  9   10           11 12   13     14   15 16      17  18   \n",
       "Tokens        Google  in  Kalifornien  ,  syn  ##tes  jeg  n  ##æste  år  \\\n",
       "Tags    ORGANIZATION   O            O  O    O      O    O  O       O   O   \n",
       "\n",
       "           19  \n",
       "Tokens  [SEP]  \n",
       "Tags        O  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide_output\n",
    "text_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien, syntes jeg næste år\"\n",
    "tag_text(text_de, tags, trainer.model, bert_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
