{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['News', 'Web', 'Conversation', 'Social Media', 'Wiki & Books',\n",
       "       'Legal', None, 'dannet'], dtype=object)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_parquet(\"data/train-00000-of-00001.parquet\")\n",
    "train[\"dagw_domain\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Den franske Revolutions Historie i kort Begreb\n",
      "\n",
      "franske : NORP\n"
     ]
    }
   ],
   "source": [
    "chosen_sent = train.iloc[28]\n",
    "print(chosen_sent[\"text\"])\n",
    "print()\n",
    "for ent in chosen_sent[\"ents\"]:\n",
    "    print(chosen_sent[\"text\"][ent[\"start\"]:ent[\"end\"]], \":\", ent[\"label\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import myutils\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set seed for consistency\n",
    "torch.manual_seed(8446)\n",
    "# Set some constants\n",
    "MLM = 'vesteinn/DanskBERT'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.00001\n",
    "EPOCHS = 3\n",
    "\n",
    "UNK = \"[UNK]\"\n",
    "MAX_TRAIN_SENTS=500\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁Hand', '▁mod', '▁Hand', ',', '▁18', '16', '▁', '</s>']\n",
      "Hand mod Hand, 1816\n",
      "\n",
      "\n",
      "['<s>', '</s>']\n",
      "['<s>', '▁Hand', '▁mod', '▁Hand', ',', '▁', '</s>']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MLM)\n",
    "chosen_sent = train.iloc[8]\n",
    "\n",
    "print(tokenizer.tokenize(tokenizer.decode(tokenizer.encode(chosen_sent[\"text\"]))))\n",
    "print(chosen_sent[\"text\"])\n",
    "print()\n",
    "for ent in chosen_sent[\"ents\"]:\n",
    "    tokens = chosen_sent[\"text\"][ent[\"start\"]:ent[\"end\"]], \":\", ent[\"label\"]\n",
    "    labels = [''] * len(tokens)\n",
    "    print(tokenizer.tokenize(tokenizer.decode(tokenizer.encode(chosen_sent[\"text\"][:ent[\"start\"]]))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '▁Taler',\n",
       " '▁9:',\n",
       " '▁også',\n",
       " '▁ret',\n",
       " '▁meget',\n",
       " '▁pas',\n",
       " ',',\n",
       " '▁men',\n",
       " '▁mest',\n",
       " '▁på',\n",
       " '▁1',\n",
       " '▁',\n",
       " '</s>']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(chosen_sent[\"text\"]).tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taler 9: også ret meget pas, men mest på 1\n",
      "\n",
      "Taler\n",
      "9\n",
      ":\n",
      "også\n",
      "ret\n",
      "meget\n",
      "pas\n",
      ",\n",
      "men\n",
      "mest\n",
      "på\n",
      "1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chosen_sent = train.iloc[1231]\n",
    "print(chosen_sent[\"text\"])\n",
    "\n",
    "for token in chosen_sent[\"tokens\"]:\n",
    "    print(chosen_sent[\"text\"][token[\"start\"]:token[\"end\"]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_and_preserve_labels(chosen_sent[\"text\"], )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
